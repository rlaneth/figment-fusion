{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![figment-fusion-logo.svg](data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMzUiIGhlaWdodD0iMjUiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDkuMjYwNCA2LjYxNDYiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgIDxnIHRyYW5zZm9ybT0ibWF0cml4KC44NjQ1NiAwIDAgLjg4NDIxIC0zMy4zNjggLTE1Ni41NykiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCI+CiAgICAgIDxnIHRyYW5zZm9ybT0idHJhbnNsYXRlKC0yOC42ODYgOTAuNjk2KSIgZmlsbD0iI2ZmNWU4ZCI+CiAgICAgICAgIDxwYXRoIGQ9Im03Ni4zNzcgODYuMzcydjUuODY1M2gtNC4yNDk5djEuNjE1NGg1Ljg2NTN2LTcuNDgwN3oiIHN0cm9rZS13aWR0aD0iLjI3MjI5Ii8+CiAgICAgICAgIDxyZWN0IHg9IjczLjQ0NCIgeT0iODkuNjAzIiB3aWR0aD0iMS4zMTczIiBoZWlnaHQ9IjEuMDE5NiIgc3Ryb2tlLXdpZHRoPSIuNDc0NjEiLz4KICAgICAgPC9nPgogICAgICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjguNjg2IDkwLjY5NikiIGZpbGw9IiM2MDlmZmYiPgogICAgICAgICA8cGF0aCBkPSJtNjguODk3IDkzLjg1M3YtNS44NjUzaDQuMjQ5OXYtMS42MTU0aC01Ljg2NTN2Ny40ODA3eiIgc3Ryb2tlLXdpZHRoPSIuMjcyMjkiLz4KICAgICAgICAgPHJlY3QgeD0iNzAuNTEyIiB5PSI4OS42MDMiIHdpZHRoPSIxLjMxNzMiIGhlaWdodD0iMS4wMTk2IiBzdHJva2Utd2lkdGg9Ii40NzQ2MSIvPgogICAgICA8L2c+CiAgIDwvZz4KPC9zdmc+)\n",
        "\n",
        "# Figment Fusion"
      ],
      "metadata": {
        "id": "vHdMAYRiDogZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar04U3MfCZ91"
      },
      "source": [
        "`v1.0.0-alpha.4`\n",
        "\n",
        "This is a pre-release version of Figment Fusion. You are welcome to share your feedback and contribute on [GitHub](https://github.com/rlaneth/figment-fusion)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsa1DVu67pJ-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X98ERicP5PK"
      },
      "source": [
        "Hello there! ðŸ‘‹\n",
        "\n",
        "You've arrived at Figment Fusion, your way into the world of AI-based image generation.\n",
        "\n",
        "This notebook is designed to provide insights on the process of running [Stable Diffusion](https://stability.ai/blog/stable-diffusion-announcement), allowing for exploration and customization while remaining simple and fun to use.\n",
        "\n",
        "Stable Diffusion is a state-of-the-art, free and open text-to-image model created by researchers from [CompVis](https://github.com/CompVis) and [Runway](https://runwayml.com/), with support from [Eleuther AI](https://www.eleuther.ai/), [LAION](https://laion.ai/) and [Stability AI](https://stability.ai/).\n",
        "\n",
        "The [Diffusers library](https://github.com/huggingface/diffusers) by Hugging Face is used here with inspiration from the _[Stable Diffusion with Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/stable_diffusion.ipynb)_ demo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1NQ5OLNfm1d"
      },
      "source": [
        "## ðŸŒ± Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7pjIGsYUmW"
      },
      "source": [
        "The steps in this notebook should be completed in the order they appear. Executing them in the wrong order might result in errors or other unexpected behavior.\n",
        "\n",
        "The following code snippet displays the GPUs available on your current runtime, assisting you in determining whether you are working in an appropriate environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xlKQv-cgd5e"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Gn4-sMY4sV"
      },
      "source": [
        "If you are running this notebook on hosted [Google Colab](https://colab.research.google.com/) runtime, you might be provided with either a NVIDIA Tesla T4 or a P100, two of the most frequently seen of the GPU models made available through the service. As according to the [Google Colab FAQ](https://research.google.com/colaboratory/faq.html), the available GPU types may vary over time and access to resources is never guaranteed.\n",
        "\n",
        "**Note:** previous versions of this notebook incorrectly stated that the Tesla P100 was faster and offered more VRAM than the T4. Both provide the same amount of VRAM, and data suggests that the T4 may be slightly faster than the P100 when generating images with Figment Fusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY99c9mkqswS"
      },
      "source": [
        "## âš™ï¸ Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uN19piSd26Zl"
      },
      "outputs": [],
      "source": [
        "#@title Model Selection\n",
        "\n",
        "MODEL_MAP = {\n",
        "  'FP16 Stable Diffusion v1.1': ['CompVis/stable-diffusion-v1-1', 'fp16'],\n",
        "  'FP16 Stable Diffusion v1.2': ['CompVis/stable-diffusion-v1-2', 'fp16'],\n",
        "  'FP16 Stable Diffusion v1.3': ['CompVis/stable-diffusion-v1-3', 'fp16'],\n",
        "  'FP16 Stable Diffusion v1.4': ['CompVis/stable-diffusion-v1-4', 'fp16'],\n",
        "  'FP16 Waifu Diffusion': ['hakurei/waifu-diffusion', 'fp16'],\n",
        "  'Stable Diffusion v1.1': ['CompVis/stable-diffusion-v1-1', 'main'],\n",
        "  'Stable Diffusion v1.2': ['CompVis/stable-diffusion-v1-2', 'main'],\n",
        "  'Stable Diffusion v1.3': ['CompVis/stable-diffusion-v1-3', 'main'],\n",
        "  'Stable Diffusion v1.4': ['CompVis/stable-diffusion-v1-4', 'main'],\n",
        "  'Waifu Diffusion': ['hakurei/waifu-diffusion', 'main']\n",
        "}\n",
        "\n",
        "#@markdown You can choose which model to use for image generation here. Consider the following\n",
        "#@markdown points for making your choice.\n",
        "\n",
        "#@markdown - Half precision (i.e. `fp16`) variants of each model require less storage space (around\n",
        "#@markdown   2.6 GB) compared to the full precision versions (around 5.2 GB), and consume less\n",
        "#@markdown   [VRAM](https://www.techtarget.com/searchstorage/definition/video-RAM).\n",
        "#@markdown - Older versions of Stable Diffusion are supported, but the latest one is generally\n",
        "#@markdown    better.\n",
        "#@markdown - [Waifu Diffusion](https://huggingface.co/hakurei/waifu-diffusion) is a model based on\n",
        "#@markdown   Stable Diffusion v1.4 and fine-tuned for anime style art.\n",
        "\n",
        "#@markdown To download each model, you must have an account on the Hugging Face website and accept\n",
        "#@markdown the terms on the page of the repository that correponds to the model you've chosen. For\n",
        "#@markdown example, in order to retrieve the Stable Diffusion v1.4 model (and its `fp16` variant),\n",
        "#@markdown you are required to accept the terms on\n",
        "#@markdown [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4).\n",
        "\n",
        "use_model = \"FP16 Stable Diffusion v1.4\" #@param [\"FP16 Stable Diffusion v1.1\", \"FP16 Stable Diffusion v1.2\", \"FP16 Stable Diffusion v1.3\", \"FP16 Stable Diffusion v1.4\", \"FP16 Waifu Diffusion\", \"Stable Diffusion v1.1\", \"Stable Diffusion v1.2\", \"Stable Diffusion v1.3\", \"Stable Diffusion v1.4\", \"Waifu Diffusion\"]\n",
        "\n",
        "model_clone_repository = MODEL_MAP[use_model][0]\n",
        "model_clone_branch = MODEL_MAP[use_model][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lw44_u3NfguE"
      },
      "outputs": [],
      "source": [
        "#@title Safety Checker\n",
        "\n",
        "#@markdown By default, if the generation model creates content which is determined to be unsafe\n",
        "#@markdown (e.g. sexually explicit), it will not be displayed or saved, being replaced with a black\n",
        "#@markdown image instead. Use this parameter to toggle this feature.\n",
        "\n",
        "enable_safety_checker = True #@param { \"type\": \"boolean\" }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DXcENt30GCyI"
      },
      "outputs": [],
      "source": [
        "#@title Storage\n",
        "\n",
        "#@markdown The value of `storage_mount_path` is set to the the mount point of Google Drive on the\n",
        "#@markdown Google Colab environment. If you are running on a different environment, you should\n",
        "#@markdown change this parameter to specify a suitable location for data storage (e.g. the root\n",
        "#@markdown of a persistent volume mounted to a virtual machine).\n",
        "storage_mount_path = '/content/drive/MyDrive' #@param { \"type\": \"string\" }\n",
        "\n",
        "#@markdown The `storage_data_path` parameter specifies a directory to store all data related to\n",
        "#@markdown Figment Fusion, including the model cache and generated images. It is relative to\n",
        "#@markdown `storage_mount_path`, meaning those two values are concatenated to obtain the full path.\n",
        "storage_data_path = '/Colab Data/Figment Fusion' #@param { \"type\": \"string\" }\n",
        "\n",
        "storage_output_path = '/output'\n",
        "storage_cache_path = '/cache'\n",
        "\n",
        "storage_data_full_path = storage_mount_path + storage_data_path\n",
        "storage_output_full_path = storage_data_full_path + storage_output_path\n",
        "storage_cache_full_path = storage_data_full_path + storage_cache_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIyb-jUuh9fz"
      },
      "source": [
        "## ðŸ‘® Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lrhr5uR2iOYp"
      },
      "outputs": [],
      "source": [
        "#@title Google Drive { vertical-output: true }\n",
        "\n",
        "#@markdown If you are running on Google Colab, the notebook requires permission to access your\n",
        "#@markdown Google Drive. For other environments, you should set `storage_mount_path` accordingly and\n",
        "#@markdown ignore this step.\n",
        "\n",
        "try:\n",
        "  from pathlib import Path\n",
        "  from google.colab import drive\n",
        "  drive_mount_path = storage_mount_path.rstrip('MyDrive')\n",
        "  Path(drive_mount_path).mkdir(parents=True, exist_ok=True)\n",
        "  drive.mount(drive_mount_path, force_remount=True)\n",
        "except ImportError:\n",
        "  print('Not running on Google Colab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zE4-PiyMhUYd"
      },
      "outputs": [],
      "source": [
        "#@title Hugging Face { vertical-output: true }\n",
        "\n",
        "#@markdown This step requires an access token to your Hugging Face account, which will be used to\n",
        "#@markdown retrieve the selected generation model. You can safely ignore it if you have already\n",
        "#@markdown downloaded the model before and wish for it to be loaded from the cache.\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "!git config --global credential.helper store\n",
        "\n",
        "try:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uin0-e6wgvjm"
      },
      "source": [
        "## ðŸ“¦ Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wI6EY030hCwd"
      },
      "outputs": [],
      "source": [
        "#@title Dependencies { vertical-output: true }\n",
        "#@markdown Ensures the presence of required Python packages and data storage directories.\n",
        "\n",
        "from pathlib import Path\n",
        "!pip install --quiet diffusers==0.3.0 transformers scipy ftfy 'ipywidgets>=7,<8'\n",
        "Path(storage_output_full_path).mkdir(parents=True, exist_ok=True)\n",
        "Path(storage_cache_full_path).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFYwcIkT20c_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Pipeline { vertical-output: true }\n",
        "#@markdown Creates the Stable Diffusion pipeline. If the generation model is not present in the\n",
        "#@markdown cache, downloading will also be performed in this step.\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "from huggingface_hub import notebook_login, whoami\n",
        "\n",
        "if model_clone_branch == 'fp16':\n",
        "  print('Selected half precision model')\n",
        "  torch_dtype = torch.float16\n",
        "else:\n",
        "  print('Selected full precision model')\n",
        "  torch_dtype = torch.float32\n",
        "\n",
        "try:\n",
        "  whoami()\n",
        "  print('Hugging Face authentication token is present')\n",
        "  local_files_only = False\n",
        "except ValueError as e:\n",
        "  print('Hugging Face authentication token not present')\n",
        "  print('Model will be loaded from cache')\n",
        "  local_files_only = True\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "  model_clone_repository,\n",
        "  revision=model_clone_branch,\n",
        "  torch_dtype=torch_dtype,\n",
        "  cache_dir=storage_cache_full_path,\n",
        "  local_files_only=local_files_only,\n",
        "  use_auth_token=(not local_files_only)\n",
        ")\n",
        "\n",
        "if not enable_safety_checker:\n",
        "  def safety_checker(images, **kwargs):\n",
        "    return images, False\n",
        "  pipe.safety_checker = safety_checker\n",
        "\n",
        "pipe = pipe.to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VzIgBbauBgn7",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Helpers\n",
        "#@markdown Defines auxiliary constants, functions and classes required by Figment Fusion.\n",
        "\n",
        "import json\n",
        "import pprint\n",
        "import time\n",
        "from diffusers import PNDMScheduler, DDIMScheduler, LMSDiscreteScheduler\n",
        "from PIL import Image\n",
        "\n",
        "# Constants\n",
        "\n",
        "STORAGE_META_PATH = '/meta'\n",
        "FF_DISPLAY_NAME = 'Figment Fusion'\n",
        "FF_VERSION = 'v1.0.0-alpha.4'\n",
        "\n",
        "# Functions\n",
        "\n",
        "def get_output_meta(batch_meta):\n",
        "  return {\n",
        "    'app': FF_DISPLAY_NAME,\n",
        "    'version': FF_VERSION,\n",
        "    'model': use_model,\n",
        "    'batch': batch_meta\n",
        "  }\n",
        "\n",
        "def save_images(base_path, images, current_batch, meta):\n",
        "  meta_path = '{}/{}/{}.json'.format(base_path, STORAGE_META_PATH, current_batch)\n",
        "  meta_detailed = get_output_meta(meta)\n",
        "  meta_serialized = json.dumps(meta_detailed, indent=2)\n",
        "  with open(meta_path, 'w') as f:\n",
        "    f.write(meta_serialized)  \n",
        "  for i, image in enumerate(images):\n",
        "    image_path = '{}/{}-{}.png'.format(base_path, current_batch, i)\n",
        "    image.save(image_path)\n",
        "\n",
        "def display_images_as_grid(images):\n",
        "  columns = len(images)\n",
        "  width, height = images[0].size\n",
        "  grid = Image.new('RGB', size=(columns * width, height))\n",
        "  grid_width, grid_height = grid.size\n",
        "  for i, image in enumerate(images):\n",
        "    grid.paste(image, box=(i % columns * width, i // columns * height))\n",
        "  display(grid)\n",
        "\n",
        "def display_images(images, as_grid = False):\n",
        "  if as_grid:\n",
        "    display_images_as_grid(images)\n",
        "    return\n",
        "  for image in images:\n",
        "    display(image)\n",
        "\n",
        "def display_meta(meta):\n",
        "  pprint.pprint(meta)\n",
        "\n",
        "# Classes\n",
        "\n",
        "class Scheduler():\n",
        "  @property\n",
        "  def scheduler(self):\n",
        "    return self.__scheduler\n",
        "\n",
        "  def __init__(self, name, *args, **kwargs):\n",
        "    self.name = name\n",
        "    self.beta_start = kwargs.get('beta_start', 0.00085)\n",
        "    self.beta_end = kwargs.get('beta_end', 0.012)\n",
        "    self.beta_schedule = kwargs.get('beta_schedule', 'scaled_linear')\n",
        "    self.ddim_clip_sample = kwargs.get('ddim_clip_sample', False)\n",
        "    self.ddim_set_alpha_to_one = kwargs.get('ddim_set_alpha_to_one', False)\n",
        "    self.ddim_eta = kwargs.get('ddim_eta', 0)\n",
        "\n",
        "    scheduler_class = PNDMScheduler\n",
        "\n",
        "    if name == 'DDIM':\n",
        "      self.__scheduler = DDIMScheduler(\n",
        "        beta_start=self.beta_start,\n",
        "        beta_end=self.beta_end,\n",
        "        beta_schedule=self.beta_schedule,\n",
        "        clip_sample=self.ddim_clip_sample,\n",
        "        set_alpha_to_one=self.ddim_set_alpha_to_one\n",
        "      )\n",
        "      return\n",
        "\n",
        "    if name == 'LMS':\n",
        "      scheduler_class = LMSDiscreteScheduler\n",
        "\n",
        "    self.__scheduler = scheduler_class(\n",
        "      beta_start=self.beta_start,\n",
        "      beta_end=self.beta_end,\n",
        "      beta_schedule=self.beta_schedule\n",
        "    )\n",
        "\n",
        "class Generator():\n",
        "  def __init__(self, pipe, *args, **kwargs):\n",
        "    self.pipe = pipe\n",
        "    self.width = kwargs.get('width', 512)\n",
        "    self.height = kwargs.get('height', 512)\n",
        "    self.num_inference_steps = kwargs.get('num_inference_steps', 50)\n",
        "    self.guidance_scale = kwargs.get('guidance_scale', 7.5)\n",
        "    self.manual_seed = kwargs.get('manual_seed', None)\n",
        "    self.__generator = torch.Generator('cuda')\n",
        "    self.__scheduler = kwargs.get('scheduler', None)\n",
        "    if self.__scheduler == None:\n",
        "      self.__scheduler = Scheduler('PNDM')\n",
        "  \n",
        "  @property\n",
        "  def scheduler(self):\n",
        "    return self.__scheduler\n",
        "  \n",
        "  def scheduler(self, value):\n",
        "    self.pipe.register_modules(scheduler=value)\n",
        "    self.__scheduler = value\n",
        "\n",
        "  def generate_latents(self, batch_size, offset):\n",
        "    size_offset = batch_size + offset\n",
        "    latents_shape = (size_offset, self.pipe.unet.in_channels, self.height // 8, self.width // 8)\n",
        "    latents = torch.randn(\n",
        "      latents_shape,\n",
        "      generator=self.__generator,\n",
        "      device='cuda'\n",
        "    )\n",
        "    return latents[offset:, ...]\n",
        "  \n",
        "  def run(self, text_prompt, images_per_batch = 1, skip_images_batch = 0):\n",
        "    pipe = self.pipe\n",
        "    generator = self.__generator\n",
        "    generator_seed = self.manual_seed\n",
        "    width = self.width\n",
        "    height = self.height\n",
        "    num_inference_steps = self.num_inference_steps\n",
        "    guidance_scale = self.guidance_scale\n",
        "    scheduler = self.__scheduler\n",
        "    ddim_eta = scheduler.ddim_eta\n",
        "\n",
        "    if generator_seed == None:\n",
        "      generator_seed = generator.seed()\n",
        "    else:\n",
        "      generator.manual_seed(generator_seed)\n",
        "    \n",
        "    latents = self.generate_latents(images_per_batch, skip_images_batch)\n",
        "\n",
        "    if isinstance(text_prompt, str):\n",
        "      prompt = [text_prompt]\n",
        "      display_prompt = prompt\n",
        "\n",
        "    if images_per_batch > 1:\n",
        "      prompt = prompt * images_per_batch\n",
        "      display_prompt = text_prompt\n",
        "\n",
        "    meta = {\n",
        "      'prompt': display_prompt,\n",
        "      'width': width,\n",
        "      'height': height,\n",
        "      'num_inference_steps': num_inference_steps,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'scheduler': {\n",
        "        'name': scheduler.name,\n",
        "        'beta_start': scheduler.beta_start,\n",
        "        'beta_end': scheduler.beta_end,\n",
        "        'beta_schedule': scheduler.beta_schedule\n",
        "      },\n",
        "      'seed': generator_seed,\n",
        "      'num_images': len(prompt)\n",
        "    }\n",
        "\n",
        "    if scheduler.name == 'DDIM':\n",
        "      meta['scheduler']['ddim_clip_sample'] = scheduler.ddim_clip_sample\n",
        "      meta['scheduler']['ddim_set_alpha_to_one'] = scheduler.ddim_clip_sample\n",
        "      meta['scheduler']['ddim_eta'] = ddim_eta\n",
        "    \n",
        "    with torch.autocast('cuda'):\n",
        "      images = pipe(\n",
        "        prompt,\n",
        "        width=width,\n",
        "        height=height,\n",
        "        num_inference_steps=num_inference_steps,\n",
        "        guidance_scale=guidance_scale,\n",
        "        latents=latents,\n",
        "        eta=ddim_eta\n",
        "      ).images\n",
        "\n",
        "    return (images, meta)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE9yMxmv-S6S"
      },
      "source": [
        "## ðŸŽ¨ Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihx2lf4I-alZ",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ### General\n",
        "\n",
        "text_prompt = '' #@param { \"type\": \"string\" }\n",
        "width = 512 #@param {type:\"slider\", min:128, max:1024, step:128}\n",
        "height = 512 #@param {type:\"slider\", min:128, max:1024, step:128}\n",
        "num_inference_steps = 50 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:30, step:0.5}\n",
        "\n",
        "#@markdown ### Seed\n",
        "\n",
        "#@markdown The `manual_seed` parameter is ignored if `use_random_seed` is enabled.\n",
        "use_random_seed = True #@param { \"type\": \"boolean\" }\n",
        "manual_seed = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Scheduler\n",
        "\n",
        "#@markdown Parameters prefixed with `ddim_` are only used for the DDIM scheduler.\n",
        "use_scheduler = 'PNDM' #@param [\"PNDM\", \"DDIM\", \"LMS\"]\n",
        "beta_start = 0.00085 #@param {type:\"number\"}\n",
        "beta_end = 0.012 #@param {type:\"number\"}\n",
        "beta_schedule = 'scaled_linear' #@param [\"linear\", \"scaled_linear\", \"squaredcos_cap_v2\"]\n",
        "ddim_clip_sample = False #@param { \"type\": \"boolean\" }\n",
        "ddim_set_alpha_to_one = False #@param { \"type\": \"boolean\" }\n",
        "ddim_eta = 0 #@param {type:\"slider\", min:0, max:1, step:0.00001}\n",
        "\n",
        "#@markdown ### Batch\n",
        "\n",
        "num_batches = 1 #@param { \"type\": \"number\" }\n",
        "images_per_batch = 1 #@param { type: \"slider\", min: 1, max: 4, step: 1 }\n",
        "skip_images_batch = 0 #@param { type: \"slider\", min: 0, max: 4, step: 1 }\n",
        "\n",
        "#@markdown ### Mutation\n",
        "\n",
        "mutate_seed = True #@param { type: \"boolean\" }\n",
        "mutate_guidance_scale = False #@param { type: \"boolean\" }\n",
        "mutate_guidance_scale_min = 7.5 #@param {type:\"slider\", min:0, max:30, step:0.5}\n",
        "mutate_guidance_scale_max = 21.5 #@param {type:\"slider\", min:0, max:30, step:0.5}\n",
        "mutate_guidance_scale_interval = 1 #@param {type:\"slider\", min:0, max:5, step:0.5}\n",
        "\n",
        "#@markdown ### Output\n",
        "\n",
        "display_output = True #@param { \"type\": \"boolean\" }\n",
        "display_batch_as_grid = True #@param { \"type\": \"boolean\" }\n",
        "display_output_meta = True #@param { \"type\": \"boolean\" }\n",
        "save_output = True #@param { \"type\": \"boolean\" }\n",
        "save_output_path = '/batch_{timestamp}' #@param { \"type\": \"string\" }\n",
        "\n",
        "timestamp_start = time.time()\n",
        "print('Starting up ...')\n",
        "\n",
        "if save_output:\n",
        "  timestamp_output = str(int(timestamp_start))\n",
        "  storage_batch_path = save_output_path.replace('{timestamp}', timestamp_output)\n",
        "  storage_batch_full_path = storage_output_full_path + storage_batch_path\n",
        "  storage_meta_full_path = storage_batch_full_path + STORAGE_META_PATH\n",
        "  Path(storage_meta_full_path).mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "if use_random_seed:\n",
        "  manual_seed = None\n",
        "\n",
        "scheduler = Scheduler(\n",
        "  use_scheduler,\n",
        "  beta_start=beta_start,\n",
        "  beta_end=beta_end,\n",
        "  beta_schedule=beta_schedule,\n",
        "  ddim_clip_sample=ddim_clip_sample,\n",
        "  ddim_set_alpha_to_one=ddim_set_alpha_to_one,\n",
        "  ddim_eta=ddim_eta\n",
        ")\n",
        "\n",
        "generator = Generator(\n",
        "  pipe,\n",
        "  width=width,\n",
        "  height=height,\n",
        "  num_inference_steps=num_inference_steps,\n",
        "  guidance_scale=guidance_scale,\n",
        "  manual_seed=manual_seed,\n",
        "  scheduler=scheduler\n",
        ")\n",
        "\n",
        "for i in range(num_batches):\n",
        "  if i > 0:\n",
        "    if not mutate_seed:\n",
        "      generator.manual_seed = meta['seed']\n",
        "    if mutate_guidance_scale:\n",
        "      guidance_scale = meta['guidance_scale'] + mutate_guidance_scale_interval\n",
        "      if guidance_scale > mutate_guidance_scale_max:\n",
        "        guidance_scale = mutate_guidance_scale_min\n",
        "      generator.guidance_scale = guidance_scale\n",
        "  \n",
        "  current_batch = i + 1\n",
        "  print('Batch {} of {}'.format(current_batch, num_batches))\n",
        "\n",
        "  images, meta = generator.run(text_prompt, images_per_batch, skip_images_batch)\n",
        "  \n",
        "  if display_output:\n",
        "    display_images(images, display_batch_as_grid)\n",
        "  if display_output_meta:\n",
        "    display_meta(meta)\n",
        "  if save_output:\n",
        "    save_images(storage_batch_full_path, images, current_batch, meta)\n",
        "\n",
        "timestamp_end = time.time()\n",
        "execution_time = timestamp_end - timestamp_start\n",
        "\n",
        "print('Finished ({} seconds)'.format(execution_time))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}