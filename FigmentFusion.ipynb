{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHdMAYRiDogZ"
      },
      "source": [
        "![figment-fusion-logo.svg](data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iVVRGLTgiPz4KPHN2ZyB3aWR0aD0iMzUiIGhlaWdodD0iMjUiIHZlcnNpb249IjEuMSIgdmlld0JveD0iMCAwIDkuMjYwNCA2LjYxNDYiIHhtbG5zPSJodHRwOi8vd3d3LnczLm9yZy8yMDAwL3N2ZyI+CiAgIDxnIHRyYW5zZm9ybT0ibWF0cml4KC44NjQ1NiAwIDAgLjg4NDIxIC0zMy4zNjggLTE1Ni41NykiIHN0cm9rZS1saW5lY2FwPSJyb3VuZCIgc3Ryb2tlLWxpbmVqb2luPSJyb3VuZCI+CiAgICAgIDxnIHRyYW5zZm9ybT0idHJhbnNsYXRlKC0yOC42ODYgOTAuNjk2KSIgZmlsbD0iI2ZmNWU4ZCI+CiAgICAgICAgIDxwYXRoIGQ9Im03Ni4zNzcgODYuMzcydjUuODY1M2gtNC4yNDk5djEuNjE1NGg1Ljg2NTN2LTcuNDgwN3oiIHN0cm9rZS13aWR0aD0iLjI3MjI5Ii8+CiAgICAgICAgIDxyZWN0IHg9IjczLjQ0NCIgeT0iODkuNjAzIiB3aWR0aD0iMS4zMTczIiBoZWlnaHQ9IjEuMDE5NiIgc3Ryb2tlLXdpZHRoPSIuNDc0NjEiLz4KICAgICAgPC9nPgogICAgICA8ZyB0cmFuc2Zvcm09InRyYW5zbGF0ZSgtMjguNjg2IDkwLjY5NikiIGZpbGw9IiM2MDlmZmYiPgogICAgICAgICA8cGF0aCBkPSJtNjguODk3IDkzLjg1M3YtNS44NjUzaDQuMjQ5OXYtMS42MTU0aC01Ljg2NTN2Ny40ODA3eiIgc3Ryb2tlLXdpZHRoPSIuMjcyMjkiLz4KICAgICAgICAgPHJlY3QgeD0iNzAuNTEyIiB5PSI4OS42MDMiIHdpZHRoPSIxLjMxNzMiIGhlaWdodD0iMS4wMTk2IiBzdHJva2Utd2lkdGg9Ii40NzQ2MSIvPgogICAgICA8L2c+CiAgIDwvZz4KPC9zdmc+)\n",
        "\n",
        "# Figment Fusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ar04U3MfCZ91"
      },
      "source": [
        "`v1.0.0-beta.2`\n",
        "\n",
        "*This is an early access version of Figment Fusion. You are welcome to provide feedback and contribute to the project on [GitHub](https://github.com/rlaneth/figment-fusion).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jsa1DVu67pJ-"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X98ERicP5PK"
      },
      "source": [
        "Hello there! ðŸ‘‹\n",
        "\n",
        "You've arrived at Figment Fusion, a notebook powered by the [Diffusers](https://github.com/huggingface/diffusers) library and pre-configured for the [Stable Diffusion](https://stability.ai/blog/stable-diffusion-announcement) and [Waifu Diffusion](https://huggingface.co/hakurei/waifu-diffusion) models. If you are a technology enthusiast and eager to get started with AI image generation, then Figment Fusion is for you.\n",
        "\n",
        "The majority of the steps should be simple and fun to explore. If you encounter any issues, please consult the documentation on the [wiki](https://github.com/rlaneth/figment-fusion/wiki) or [open an issue](https://github.com/rlaneth/figment-fusion/wiki)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1NQ5OLNfm1d"
      },
      "source": [
        "## ðŸŒ± Getting Started"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pN7pjIGsYUmW"
      },
      "source": [
        "The steps in this notebook should be completed in the order they appear. Executing them in the wrong order might result in errors or other unexpected behavior.\n",
        "\n",
        "The following code snippet displays the GPUs available on your current runtime, assisting you in determining whether you are working in an appropriate environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xlKQv-cgd5e"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Gn4-sMY4sV"
      },
      "source": [
        "If you are running this notebook on [Google Colaboratory](https://colab.research.google.com/), you might be provided with either a NVIDIA Tesla T4 or a P100, two of the most frequently seen of the GPU models made available through the service. As according to the [Google Colab FAQ](https://research.google.com/colaboratory/faq.html), the available GPU types may vary over time and access to resources is never guaranteed.\n",
        "\n",
        "**Note:** previous versions of this notebook incorrectly stated that the Tesla P100 was faster and offered more VRAM than the T4. Both provide the same amount of VRAM, and data suggests that the T4 may be slightly faster than the P100 when generating images with Figment Fusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mY99c9mkqswS"
      },
      "source": [
        "## âš™ï¸ Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "uN19piSd26Zl"
      },
      "outputs": [],
      "source": [
        "#@title Model Selection\n",
        "\n",
        "\n",
        "MODEL_MAP = {\n",
        "    'Stable Diffusion v1.1': ['CompVis/stable-diffusion-v1-1', 'main'],\n",
        "    'Stable Diffusion v1.2': ['CompVis/stable-diffusion-v1-2', 'main'],\n",
        "    'Stable Diffusion v1.3': ['CompVis/stable-diffusion-v1-3', 'main'],\n",
        "    'Stable Diffusion v1.4': ['CompVis/stable-diffusion-v1-4', 'main'],\n",
        "    'Stable Diffusion v1.1 | FP16': ['CompVis/stable-diffusion-v1-1', 'fp16'],\n",
        "    'Stable Diffusion v1.2 | FP16': ['CompVis/stable-diffusion-v1-2', 'fp16'],\n",
        "    'Stable Diffusion v1.3 | FP16': ['CompVis/stable-diffusion-v1-3', 'fp16'],\n",
        "    'Stable Diffusion v1.4 | FP16': ['CompVis/stable-diffusion-v1-4', 'fp16'],\n",
        "    'Waifu Diffusion v1.2': ['hakurei/waifu-diffusion', '1692f03d5c0ab460f036adfc99a2442e8b046c12'],\n",
        "    'Waifu Diffusion v1.3': ['hakurei/waifu-diffusion', '2dff9dab944d77470b545a1bbe86fffb08d54912'],\n",
        "    'Waifu Diffusion v1.2 | FP16': ['hakurei/waifu-diffusion', '44d0d4d72a028ff9ac4e97aed4af966e4e6d90d8'],\n",
        "    'Waifu Diffusion v1.3 | FP16': ['hakurei/waifu-diffusion', '342d18da939534326d8571b7a8b5195f43800db6']\n",
        "}\n",
        "\n",
        "#@markdown You can choose which model to use for image generation here. Consider the following\n",
        "#@markdown points for making your choice.\n",
        "\n",
        "#@markdown - Half precision (`fp16`) variants of each model require less storage space (around\n",
        "#@markdown   2.6 GB) compared to the full precision versions (around 5.2 GB), run faster and consume\n",
        "#@markdown   less [VRAM](https://www.techtarget.com/searchstorage/definition/video-RAM).\n",
        "#@markdown - Older versions of Stable Diffusion are supported, but the latest one is generally\n",
        "#@markdown    better.\n",
        "#@markdown - [Waifu Diffusion](https://huggingface.co/hakurei/waifu-diffusion) is a model based on\n",
        "#@markdown   Stable Diffusion v1.4 and fine-tuned for anime style art.\n",
        "\n",
        "#@markdown To download each model, you must have an account on the Hugging Face website and accept\n",
        "#@markdown the terms on the page of the repository that correponds to the model you've chosen. For\n",
        "#@markdown example, in order to retrieve the Stable Diffusion v1.4 model (and its `fp16` variant),\n",
        "#@markdown you are required to accept the terms on\n",
        "#@markdown [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4).\n",
        "\n",
        "use_model = \"Stable Diffusion v1.4 | FP16\" #@param [\"Stable Diffusion v1.1\", \"Stable Diffusion v1.2\", \"Stable Diffusion v1.3\", \"Stable Diffusion v1.4\", \"Stable Diffusion v1.1 | FP16\", \"Stable Diffusion v1.2 | FP16\", \"Stable Diffusion v1.3 | FP16\", \"Stable Diffusion v1.4 | FP16\", \"Waifu Diffusion v1.2\", \"Waifu Diffusion v1.3\",  \"Waifu Diffusion v1.2 | FP16\", \"Waifu Diffusion v1.3 | FP16\"]\n",
        "\n",
        "model_select = MODEL_MAP[use_model]\n",
        "model_fp16 = 'FP16' in use_model\n",
        "model_repository = model_select[0]\n",
        "model_revision = model_select[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "9Re1iCsl117y"
      },
      "outputs": [],
      "source": [
        "#@title Concepts { vertical-output: true }\n",
        "\n",
        "#@markdown Here you can specify [textual inversion](https://huggingface.co/docs/diffusers/training/text_inversion)\n",
        "#@markdown concepts available on Hugging Face to be loaded. Please note that if you do so, the\n",
        "#@markdown _Hugging Face_ step under _Permissions_ will be required.\n",
        "\n",
        "from IPython.display import Markdown\n",
        "\n",
        "#@markdown If you wish to load more than one concept, you may either run this step once providing a\n",
        "#@markdown comma-separated list of repository IDs to `concepts_repo_id`, or run it more than once\n",
        "#@markdown providing one or more IDs at each time to build up a list.\n",
        "concepts_repo_id = '' #@param { \"type\": \"string\" }\n",
        "\n",
        "#@markdown To clear the existing list of concepts to be loaded, run with the\n",
        "#@markdown `concepts_reset_repo_id_list` option enabled.\n",
        "concepts_reset_repo_id_list = False #@param { \"type\": \"boolean\" }\n",
        "\n",
        "#@markdown **Hint:** to find concepts, try the\n",
        "#@markdown [Stable Diffusion concepts library](https://huggingface.co/sd-concepts-library)\n",
        "#@markdown community.\n",
        "\n",
        "if not 'concepts_repo_id_list' in locals() or concepts_reset_repo_id_list:\n",
        "  concepts_repo_id_list = []\n",
        "\n",
        "if concepts_repo_id != '':\n",
        "  concepts_repo_id_list_create = [item.strip() for item in concepts_repo_id.split(',')]\n",
        "  concepts_repo_id_list.extend(concepts_repo_id_list_create)\n",
        "\n",
        "if len(concepts_repo_id_list):\n",
        "  display(Markdown('**The concepts from the repositories with the following IDs will be loaded**'))\n",
        "  for concept_repo_id in concepts_repo_id_list:\n",
        "    print(f'- {concept_repo_id}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lw44_u3NfguE"
      },
      "outputs": [],
      "source": [
        "#@title Safety Checker\n",
        "\n",
        "#@markdown By default, if the generation model creates content which is determined to be unsafe\n",
        "#@markdown (e.g. sexually explicit), it will not be displayed or saved, being replaced with a black\n",
        "#@markdown image instead. Use this parameter to toggle this feature.\n",
        "\n",
        "enable_safety_checker = True #@param { \"type\": \"boolean\" }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "DXcENt30GCyI"
      },
      "outputs": [],
      "source": [
        "#@title Storage\n",
        "\n",
        "#@markdown The value of `storage_mount_path` is set to the the mount point of Google Drive on the\n",
        "#@markdown Google Colab environment. If you are running on a different environment, you should\n",
        "#@markdown change this parameter to specify a suitable location for data storage (e.g. the root\n",
        "#@markdown of a persistent volume mounted to a virtual machine).\n",
        "storage_mount_path = '/content/drive/MyDrive' #@param { \"type\": \"string\" }\n",
        "\n",
        "#@markdown The `storage_data_path` parameter specifies a directory to store all data related to\n",
        "#@markdown Figment Fusion, including the model cache and generated images. It is relative to\n",
        "#@markdown `storage_mount_path`, meaning those two values are concatenated to obtain the full path.\n",
        "storage_data_path = '/Colab Data/Figment Fusion' #@param { \"type\": \"string\" }\n",
        "\n",
        "storage_output_path = '/output'\n",
        "storage_cache_path = '/cache'\n",
        "\n",
        "storage_data_full_path = storage_mount_path + storage_data_path\n",
        "storage_output_full_path = storage_data_full_path + storage_output_path\n",
        "storage_cache_full_path = storage_data_full_path + storage_cache_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIyb-jUuh9fz"
      },
      "source": [
        "## ðŸ‘® Permissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "lrhr5uR2iOYp"
      },
      "outputs": [],
      "source": [
        "#@title Google Drive { vertical-output: true }\n",
        "\n",
        "#@markdown If you are running on Google Colab, the notebook requires permission to access your\n",
        "#@markdown Google Drive. For other environments, you should set `storage_mount_path` accordingly and\n",
        "#@markdown ignore this step.\n",
        "\n",
        "try:\n",
        "  from pathlib import Path\n",
        "  from google.colab import drive\n",
        "  drive_mount_path = storage_mount_path.rstrip('MyDrive')\n",
        "  Path(drive_mount_path).mkdir(parents=True, exist_ok=True)\n",
        "  drive.mount(drive_mount_path, force_remount=True)\n",
        "except ImportError:\n",
        "  print('Not running on Google Colab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zE4-PiyMhUYd"
      },
      "outputs": [],
      "source": [
        "#@title Hugging Face { vertical-output: true }\n",
        "\n",
        "#@markdown This step requires an access token to your Hugging Face account, which will be used to\n",
        "#@markdown retrieve the selected generation model. You can safely ignore it if you have already\n",
        "#@markdown downloaded the model before and wish for it to be loaded from the cache.\n",
        "\n",
        "!pip install --quiet huggingface_hub\n",
        "!git config --global credential.helper store\n",
        "\n",
        "try:\n",
        "  from google.colab import output\n",
        "  output.enable_custom_widget_manager()\n",
        "except ImportError:\n",
        "  pass\n",
        "\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uin0-e6wgvjm"
      },
      "source": [
        "## ðŸ“¦ Requirements"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzjxSPEimacp"
      },
      "source": [
        "The cells under the _Requirements_ section should not require your interaction (unless something goes wrong). You can simply run all of these steps and, if no errors occur, you will be ready to continue."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "hJYN_zhUrutP"
      },
      "outputs": [],
      "source": [
        "#@title Initialization { vertical-output: true }\n",
        "#@markdown Installs packages, imports modules and defines constants and variables required by the\n",
        "#@markdown following steps.\n",
        "\n",
        "# Packages\n",
        "!pip install --quiet diffusers==0.4.1 transformers scipy ftfy 'ipywidgets>=7,<8'\n",
        "\n",
        "# Imports\n",
        "import json\n",
        "import pprint\n",
        "import time\n",
        "import torch\n",
        "from diffusers import DDIMScheduler, LMSDiscreteScheduler, PNDMScheduler, StableDiffusionPipeline\n",
        "from huggingface_hub import hf_hub_download, notebook_login, whoami\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "# Constants\n",
        "STORAGE_META_PATH = '/meta'\n",
        "FF_DISPLAY_NAME = 'Figment Fusion'\n",
        "FF_VERSION = 'v1.0.0-beta.2'\n",
        "\n",
        "# Create paths\n",
        "Path(storage_output_full_path).mkdir(parents=True, exist_ok=True)\n",
        "Path(storage_cache_full_path).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Variables\n",
        "if model_fp16:\n",
        "  torch_dtype = torch.float16\n",
        "else:\n",
        "  torch_dtype = torch.float32\n",
        "\n",
        "try:\n",
        "  whoami()\n",
        "  local_files_only = False\n",
        "except: \n",
        "  local_files_only = True\n",
        "\n",
        "# Output\n",
        "display(\n",
        "  Markdown('- **Model precision:** {}'.format('fp16' if torch_dtype == torch.float16 else 'fp32')),\n",
        "  Markdown('- **Hugging Face token:** {}'.format('not present' if local_files_only else 'present'))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mzDfxqSWpn4Q"
      },
      "outputs": [],
      "source": [
        "#@title Concepts { vertical-output: true }\n",
        "\n",
        "#@markdown Downloads the concept files from the repositories specified in the _Concepts_ step under\n",
        "#@markdown _Settings_.\n",
        "\n",
        "concepts_required_files = ['learned_embeds.bin', 'token_identifier.txt']\n",
        "concepts_meta = []\n",
        "\n",
        "for repo_id in concepts_repo_id_list:\n",
        "  meta_files = {}\n",
        "  for filename in concepts_required_files:\n",
        "    meta_files[filename] = hf_hub_download(\n",
        "      repo_id=repo_id,\n",
        "      filename=filename,\n",
        "      cache_dir=storage_cache_full_path,\n",
        "      local_files_only=local_files_only,\n",
        "      use_auth_token=use_auth_token\n",
        "    )\n",
        "  concepts_meta.append({\n",
        "    'name': repo_id,\n",
        "    'files': meta_files\n",
        "  })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "VzIgBbauBgn7"
      },
      "outputs": [],
      "source": [
        "#@title Helpers\n",
        "#@markdown Defines auxiliary functions and classes required by the following steps.\n",
        "\n",
        "def load_concept(tokenizer, text_encoder, path):\n",
        "  concept = torch.load(path, map_location='cpu')\n",
        "\n",
        "  trained_token = list(concept.keys())[0]\n",
        "  embeds = concept[trained_token]\n",
        "\n",
        "  dtype = text_encoder.get_input_embeddings().weight.dtype\n",
        "  embeds.to(dtype)\n",
        "\n",
        "  added_tokens = tokenizer.add_tokens(trained_token)\n",
        "  if added_tokens == 0:\n",
        "    raise ValueError(f'The tokenizer already contains the token {trained_token}')\n",
        "\n",
        "  text_encoder.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "  token_id = tokenizer.convert_tokens_to_ids(trained_token)\n",
        "  text_encoder.get_input_embeddings().weight.data[token_id] = embeds\n",
        "\n",
        "  return trained_token\n",
        "\n",
        "def get_output_meta(batch_meta):\n",
        "  return {\n",
        "    'app': FF_DISPLAY_NAME,\n",
        "    'version': FF_VERSION,\n",
        "    'model': use_model,\n",
        "    'batch': batch_meta\n",
        "  }\n",
        "\n",
        "def save_images(base_path, images, current_batch, meta):\n",
        "  meta_path = '{}/{}/{}.json'.format(base_path, STORAGE_META_PATH, current_batch)\n",
        "  meta_detailed = get_output_meta(meta)\n",
        "  meta_serialized = json.dumps(meta_detailed, indent=2)\n",
        "  with open(meta_path, 'w') as f:\n",
        "    f.write(meta_serialized)  \n",
        "  for i, image in enumerate(images):\n",
        "    image_path = '{}/{}-{}.png'.format(base_path, current_batch, i + 1)\n",
        "    image.save(image_path)\n",
        "\n",
        "def display_images_as_grid(images):\n",
        "  columns = len(images)\n",
        "  width, height = images[0].size\n",
        "  grid = Image.new('RGB', size=(columns * width, height))\n",
        "  grid_width, grid_height = grid.size\n",
        "  for i, image in enumerate(images):\n",
        "    grid.paste(image, box=(i % columns * width, i // columns * height))\n",
        "  display(grid)\n",
        "\n",
        "def display_images(images, as_grid = False):\n",
        "  if as_grid:\n",
        "    display_images_as_grid(images)\n",
        "    return\n",
        "  for image in images:\n",
        "    display(image)\n",
        "\n",
        "def display_meta(meta):\n",
        "  pprint.pprint(meta)\n",
        "\n",
        "class Scheduler():\n",
        "  @property\n",
        "  def scheduler(self):\n",
        "    return self.__scheduler\n",
        "\n",
        "  def __init__(self, name, *args, **kwargs):\n",
        "    self.name = name\n",
        "    self.beta_start = kwargs.get('beta_start', 0.00085)\n",
        "    self.beta_end = kwargs.get('beta_end', 0.012)\n",
        "    self.beta_schedule = kwargs.get('beta_schedule', 'scaled_linear')\n",
        "    self.ddim_clip_sample = kwargs.get('ddim_clip_sample', False)\n",
        "    self.ddim_set_alpha_to_one = kwargs.get('ddim_set_alpha_to_one', False)\n",
        "    self.ddim_eta = kwargs.get('ddim_eta', 0)\n",
        "\n",
        "    scheduler_class = PNDMScheduler\n",
        "\n",
        "    if name == 'DDIM':\n",
        "      self.__scheduler = DDIMScheduler(\n",
        "        beta_start=self.beta_start,\n",
        "        beta_end=self.beta_end,\n",
        "        beta_schedule=self.beta_schedule,\n",
        "        clip_sample=self.ddim_clip_sample,\n",
        "        set_alpha_to_one=self.ddim_set_alpha_to_one\n",
        "      )\n",
        "      return\n",
        "\n",
        "    if name == 'LMS':\n",
        "      scheduler_class = LMSDiscreteScheduler\n",
        "\n",
        "    self.__scheduler = scheduler_class(\n",
        "      beta_start=self.beta_start,\n",
        "      beta_end=self.beta_end,\n",
        "      beta_schedule=self.beta_schedule\n",
        "    )\n",
        "\n",
        "class Generator():\n",
        "  def __init__(self, pipe, *args, **kwargs):\n",
        "    self.pipe = pipe\n",
        "    self.width = kwargs.get('width', 512)\n",
        "    self.height = kwargs.get('height', 512)\n",
        "    self.num_inference_steps = kwargs.get('num_inference_steps', 50)\n",
        "    self.guidance_scale = kwargs.get('guidance_scale', 7.5)\n",
        "    self.manual_seed = kwargs.get('manual_seed', None)\n",
        "    self.__generator = torch.Generator('cuda')\n",
        "    self.__scheduler = kwargs.get('scheduler', None)\n",
        "    if self.__scheduler == None:\n",
        "      self.__scheduler = Scheduler('PNDM')\n",
        "  \n",
        "  @property\n",
        "  def scheduler(self):\n",
        "    return self.__scheduler\n",
        "  \n",
        "  def scheduler(self, value):\n",
        "    self.pipe.register_modules(scheduler=value)\n",
        "    self.__scheduler = value\n",
        "\n",
        "  def generate_latents(self, batch_size, offset):\n",
        "    size_offset = batch_size + offset\n",
        "    latents_shape = (size_offset, self.pipe.unet.in_channels, self.height // 8, self.width // 8)\n",
        "    latents = torch.randn(\n",
        "      latents_shape,\n",
        "      generator=self.__generator,\n",
        "      dtype=self.pipe.text_encoder.get_input_embeddings().weight.dtype,\n",
        "      device='cuda'\n",
        "    )\n",
        "    return latents[offset:, ...]\n",
        "  \n",
        "  def run(self, text_prompt, negative_text_prompt, images_per_batch = 1, offset = 0):\n",
        "    pipe = self.pipe\n",
        "    generator = self.__generator\n",
        "    generator_seed = self.manual_seed\n",
        "    width = self.width\n",
        "    height = self.height\n",
        "    num_inference_steps = self.num_inference_steps\n",
        "    guidance_scale = self.guidance_scale\n",
        "    scheduler = self.__scheduler\n",
        "    ddim_eta = scheduler.ddim_eta\n",
        "\n",
        "    if generator_seed == None:\n",
        "      generator_seed = generator.seed()\n",
        "    else:\n",
        "      generator.manual_seed(generator_seed)\n",
        "    \n",
        "    latents = self.generate_latents(images_per_batch, offset)\n",
        "\n",
        "    if negative_text_prompt == '':\n",
        "      negative_prompt = None\n",
        "\n",
        "    meta = {\n",
        "      'prompt': text_prompt,\n",
        "      'negative_prompt': negative_text_prompt,\n",
        "      'width': width,\n",
        "      'height': height,\n",
        "      'num_inference_steps': num_inference_steps,\n",
        "      'guidance_scale': guidance_scale,\n",
        "      'scheduler': scheduler.name,\n",
        "      'seed': generator_seed,\n",
        "      'num_images': images_per_batch\n",
        "    }\n",
        "\n",
        "    images = pipe(\n",
        "      text_prompt,\n",
        "      negative_prompt=negative_text_prompt,\n",
        "      num_images_per_prompt=images_per_batch,\n",
        "      width=width,\n",
        "      height=height,\n",
        "      num_inference_steps=num_inference_steps,\n",
        "      guidance_scale=guidance_scale,\n",
        "      latents=latents,\n",
        "      eta=ddim_eta\n",
        "    ).images\n",
        "\n",
        "    return (images, meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "qFYwcIkT20c_"
      },
      "outputs": [],
      "source": [
        "#@title Pipeline { vertical-output: true }\n",
        "\n",
        "#@markdown Creates the Stable Diffusion pipeline. If the generation model is not in the cache,\n",
        "#@markdown downloading will take place in this step as well.\n",
        "\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\n",
        "  model_repository,\n",
        "  revision=model_revision,\n",
        "  subfolder='tokenizer',\n",
        "  cache_dir=storage_cache_full_path,\n",
        "  local_files_only=local_files_only\n",
        ") \n",
        "\n",
        "text_encoder = CLIPTextModel.from_pretrained(\n",
        "  model_repository,\n",
        "  revision=model_revision,\n",
        "  subfolder='text_encoder',\n",
        "  torch_dtype=torch_dtype,\n",
        "  cache_dir=storage_cache_full_path,\n",
        "  local_files_only=local_files_only\n",
        ")\n",
        "\n",
        "for meta in concepts_meta:\n",
        "  concepts_name = meta['name']\n",
        "  concepts_token = load_concept(\n",
        "    tokenizer,\n",
        "    text_encoder,\n",
        "    meta['files']['learned_embeds.bin']\n",
        "  )\n",
        "  print(f'Loaded {concepts_name} as {concepts_token}')\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "  model_repository,\n",
        "  revision=model_revision,\n",
        "  torch_dtype=torch_dtype,\n",
        "  text_encoder=text_encoder,\n",
        "  tokenizer=tokenizer,\n",
        "  cache_dir=storage_cache_full_path,\n",
        "  local_files_only=local_files_only\n",
        ").to('cuda')\n",
        "\n",
        "if not enable_safety_checker:\n",
        "  def safety_checker(images, **kwargs):\n",
        "    return images, False\n",
        "  pipe.safety_checker = safety_checker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uE9yMxmv-S6S"
      },
      "source": [
        "## ðŸŽ¨ Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ihx2lf4I-alZ"
      },
      "outputs": [],
      "source": [
        "#@markdown ### General\n",
        "\n",
        "text_prompt = '' #@param { \"type\": \"string\" }\n",
        "negative_text_prompt = '' #@param { \"type\": \"string\" }\n",
        "width = 512 #@param {type:\"slider\", min:128, max:1024, step:128}\n",
        "height = 512 #@param {type:\"slider\", min:128, max:1024, step:128}\n",
        "num_inference_steps = 50 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "guidance_scale = 7.5 #@param {type:\"slider\", min:0, max:30, step:0.5}\n",
        "\n",
        "#@markdown ### Seed\n",
        "\n",
        "#@markdown **Note:** the `manual_seed` parameter is ignored if `use_random_seed` is enabled.\n",
        "use_random_seed = True #@param { \"type\": \"boolean\" }\n",
        "manual_seed = 0 #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown ### Scheduler\n",
        "\n",
        "use_scheduler = 'PNDM' #@param [\"PNDM\", \"DDIM\", \"LMS\"]\n",
        "\n",
        "#@markdown ### Batch\n",
        "\n",
        "num_batches = 1 #@param { \"type\": \"number\" }\n",
        "images_per_batch = 1 #@param { type: \"slider\", min: 1, max: 4, step: 1 }\n",
        "offset_batch = 0 #@param { type: \"slider\", min: 0, max: 4, step: 1 }\n",
        "\n",
        "#@markdown ### Mutation\n",
        "\n",
        "mutate_seed = True #@param { type: \"boolean\" }\n",
        "mutate_guidance_scale = False #@param { type: \"boolean\" }\n",
        "mutate_guidance_scale_min = 7.5 #@param {type:\"slider\", min:0, max:30, step:0.5}\n",
        "mutate_guidance_scale_max = 21.5 #@param {type:\"slider\", min:0, max:30, step:0.5}\n",
        "mutate_guidance_scale_interval = 1 #@param {type:\"slider\", min:0, max:5, step:0.5}\n",
        "\n",
        "#@markdown ### Output\n",
        "\n",
        "display_output = True #@param { \"type\": \"boolean\" }\n",
        "display_batch_as_grid = True #@param { \"type\": \"boolean\" }\n",
        "display_output_meta = True #@param { \"type\": \"boolean\" }\n",
        "save_output = True #@param { \"type\": \"boolean\" }\n",
        "save_output_path = '/run_{timestamp}' #@param { \"type\": \"string\" }\n",
        "\n",
        "timestamp_start = time.time()\n",
        "print('Starting up ...')\n",
        "\n",
        "if save_output:\n",
        "  timestamp_output = str(int(timestamp_start))\n",
        "  storage_batch_path = save_output_path.replace('{timestamp}', timestamp_output)\n",
        "  storage_batch_full_path = storage_output_full_path + storage_batch_path\n",
        "  storage_meta_full_path = storage_batch_full_path + STORAGE_META_PATH\n",
        "  Path(storage_meta_full_path).mkdir(parents=True, exist_ok=False)\n",
        "\n",
        "if use_random_seed:\n",
        "  manual_seed = None\n",
        "\n",
        "scheduler = Scheduler(use_scheduler)\n",
        "\n",
        "generator = Generator(\n",
        "  pipe,\n",
        "  width=width,\n",
        "  height=height,\n",
        "  num_inference_steps=num_inference_steps,\n",
        "  guidance_scale=guidance_scale,\n",
        "  manual_seed=manual_seed,\n",
        "  scheduler=scheduler\n",
        ")\n",
        "\n",
        "for i in range(num_batches):\n",
        "  if i > 0:\n",
        "    if not mutate_seed:\n",
        "      generator.manual_seed = meta['seed']\n",
        "    if mutate_guidance_scale:\n",
        "      guidance_scale = meta['guidance_scale'] + mutate_guidance_scale_interval\n",
        "      if guidance_scale > mutate_guidance_scale_max:\n",
        "        guidance_scale = mutate_guidance_scale_min\n",
        "      generator.guidance_scale = guidance_scale\n",
        "  \n",
        "  current_batch = i + 1\n",
        "  print('Batch {} of {}'.format(current_batch, num_batches))\n",
        "\n",
        "  images, meta = generator.run(text_prompt, negative_text_prompt, images_per_batch, offset_batch)\n",
        "  \n",
        "  if display_output:\n",
        "    display_images(images, display_batch_as_grid)\n",
        "  if display_output_meta:\n",
        "    display_meta(meta)\n",
        "  if save_output:\n",
        "    save_images(storage_batch_full_path, images, current_batch, meta)\n",
        "\n",
        "timestamp_end = time.time()\n",
        "execution_time = timestamp_end - timestamp_start\n",
        "\n",
        "print('Finished ({} seconds)'.format(execution_time))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "private_outputs": true,
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
